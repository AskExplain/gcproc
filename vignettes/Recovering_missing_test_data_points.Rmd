---
title: "Recovering missing test data points"
output:
  html_document:
    theme: united
  pdf_document: default
date: 'Compiled: `r format(Sys.Date(), "%B %d, %Y")`'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Introduction

Generalised Canonical Procrustes (gcproc) can recover missing values in a dataset via a prediction or imputation step, depending on the missingness of the data. Recovery begins by projecting the features of the data into a reduced dimensional subspace: encoding to improve information quality. When both the predictor (y) and feature (x) variables are transformed via the encoding projection learned in gcproc, a linear model is used to reweight the feature to find an estimate of the predictor. The key step in this process is learning a set of parameters via gcproc that encode the features to enrich the information quality before input into the linear model regression.

### Step 0 - Prepare libraries

```{r step_0}
library(gcproc,quietly = T)
library(mclust,quietly = T)
library(irlba,quietly = T)

```


### Step 1 - Initialise config
```{r step_1}

config.predict <- gcproc::extract_config(T)
config.predict$i_dim <- 30
config.predict$tol <- 1e-3
config.predict$verbose <- F
config.predict$init <- "svd-quick"

```


### Step 2 - Prepare data

```{r step_2}

predict.list <- list()
predict.names <- c("wine","diamonds","golub","oliveoil")

library(pdfCluster)
data("wine")
predict.list[[1]] <- as.matrix(wine[,-1])

library(glmnet)
data("QuickStartExample")
predict.list[[2]] <- cbind(QuickStartExample$y,QuickStartExample$x)

library(multtest)
data("golub")
predict.list[[3]] <- golub[,order(apply(golub,2,var),decreasing = T)[1:10]]

library(pdfCluster)
data("oliveoil")
predict.list[[4]] <- as.matrix(oliveoil[,-c(1,2)])



```






### Step 3 - Run gcproc, glmnet and linear model on test datasets

Three models are run: gcproc, glmnet and lm - respectively they are Generalised Canonical Procrustes, a Regularised Generalised Linear Model, and a Standard Linear Model. On smaller datasets, it is expected for a linear model to perform well, whereas on larger datasets with more features, it is expected a regularised method such as glmnet or gcproc to perform well.  The root mean squared error is reported, along with the pearson correlation.

It should be mentioned that by encoding the sample and feature space simultaneously, gcproc acts similar to a regularized method without the optimization of a penalty term usually called the regularization penalty. It does this by encoding the samples, which reduces the effective number of "data points" once projected and constrains the feature parameter in gcproc to find a suitable transformation that reflects the enhanced information quality via the encoding and noise reduction in the sample parameter. It is for this reason it is expected to do well on larger datasets.

Four datasets are used in this regression to predict missing values. The most variable feature is listed as the variable to be predicted, while all other variables are listed as the covariates.

```{r step_3,warning=FALSE}

for (list_id in c(1:length(predict.list))){
  # Take dataset
  data___x <- data.frame(predict.list[[list_id]])
  
  # Find most variable feature
  y_predict.id_col <- tail(order(apply(data___x,2,var)),1)
  
  main_scores.1 <- c()
  
  for (seed in c(1:10)){
    
    # Set up training ids
    set.seed(seed)
    train_id <- sample(c(1:dim(data___x)[1]),size = dim(data___x)[1]*0.9)
    
    # Set up y column by looping through each column
    
    # glmnet
    glmnet_predict <- predict(glmnet::cv.glmnet(x=as.matrix(data___x[train_id,-c(y_predict.id_col)]),y=as.matrix(data___x[train_id,y_predict.id_col]),type.measure = "mse"),as.matrix(data___x[-train_id,-c(y_predict.id_col)]), s = "lambda.min")
    
    # lm
    lm_predict <- predict(lm(as.matrix(data___x[train_id,y_predict.id_col])~.,data=data.frame(x=data___x[train_id,-c(y_predict.id_col)])),data.frame(x=data___x[-train_id,-c(y_predict.id_col)]))
    
    
    # Set up gcproc
    
    # Set up prediction for gcproc
    predict <- gcproc::extract_prediction_framework(F)
    predict$x <- predict$y <- array(0,dim=dim(data___x))
    predict$x[-train_id,y_predict.id_col] <- predict$y[-train_id,y_predict.id_col] <- 1
    
    # Initialise missing values given by prediction design matrix with median feature value
    x.x.check <- as.matrix(data___x)
    x.x.check <- x.x.check*(1-predict$y) + do.call('cbind',lapply(c(1:dim(x.x.check*(1-predict$y))[2]),function(X){(predict$y[,X])*median(x.x.check[which((1-predict$y)[,X]==1),X])}))
    
    # Run gcproc
    gcproc.model <- gcproc::gcproc(x=x.x.check,
                                   y=x.x.check,
                                   config = config.predict,
                                   predict = predict,
                                   fixed = list(i_dim = T, j_dim = T),
                                   reference = "x"
    )
    
    
    # Extract predicted values
    gcproc.predict <- gcproc.model$predict$predict.y
    
    y_hat.plot <- gcproc.predict[which(predict$y==1,arr.ind = T)]
    data___x.plot <- data___x[which((predict$y)==1,arr.ind = T)]
    
    # Set up table for plotting
    vals <- data.frame(rbind(data.frame(seed,"Feature Column"=colnames(data___x)[y_predict.id_col],Metric="Loss: Root Mean Squared Error",
                                        data.frame(Method = c("gcproc","glmnet","linear model"),Value=c(mean(sqrt((y_hat.plot-data___x.plot)^2)),mean(sqrt((glmnet_predict-data___x.plot)^2)),mean(sqrt((lm_predict-data___x.plot)^2))))),
                             data.frame(seed,"Feature Column"=colnames(data___x)[y_predict.id_col],Metric="Correlation: Pearson",                                data.frame(Method=c("gcproc","glmnet","linear model"),Value=c(cor(y_hat.plot,data___x.plot),cor(glmnet_predict,data___x.plot),cor(lm_predict,data___x.plot)))
                             )
    ))
    
    main_scores.1 <- rbind(main_scores.1,vals)
    
    
  }
  
  
  library(ggplot2)
  a <- ggplot((main_scores.1[main_scores.1$Metric=="Loss: Root Mean Squared Error",]), aes(x=Metric, y=Value, fill=Method))+ 
    geom_violin() + ggtitle(label = predict.names[list_id])
    print(a)

  
  library(ggplot2)
  b <- ggplot((main_scores.1[main_scores.1$Metric=="Correlation: Pearson",]), aes(x=Metric, y=Value, fill=Method)) +
    geom_violin() + ggtitle(label = predict.names[list_id]) 
    print(b)

  
}



```







### Step 4 - Run gcproc and mice on various datasets with missigness via ampute

To induce missingness into the dataset, the default "missing at random" was induced via the ampute function via mice. This is in contrast to other types of missingness that can be [not at random], or [completely at random]. For gcproc, the missing data is initialized via the median value for the corresponding feature. The root mean squared error is reported, along with the pearson correlation.

```{r step_4,warning=FALSE}


for (list_id in c(1:length(predict.list))){
  # Take dataset
  data___x <- predict.list[[list_id]]

  main_scores.2<-c()
  
  for (seed in 1:3){
    
    set.seed(seed)
    
    # Run mice
    data___x.check <- (as.matrix(as.data.frame(data___x)))
    data___x.check <- mice::ampute(data = data___x.check,mech = "MAR")
    data___x.check <- mice::mice(data___x.check$amp,printFlag = F)
    mice.pred <- mice::complete(data___x.check)
    
    # Set up prediction design matrix
    predict <- gcproc::extract_prediction_framework(F)
    predict$y <- predict$x <- (data___x.check$where)
    
    # Initialise missing values given by prediction design matrix with median feature value
    x.x.check <- as.matrix(data___x)
    x.x.check <- x.x.check*(1-predict$y) + do.call('cbind',lapply(c(1:dim(x.x.check*(1-predict$y))[2]),function(X){(predict$y[,X])*median(x.x.check[which((1-predict$y)[,X]==1),X])}))
    
    # Run gcproc
    gcproc.model <- gcproc::gcproc(x=x.x.check,
                                   y=x.x.check,
                                   config = config.predict,
                                   predict = predict,
                                   fixed = list(i_dim = T, j_dim = T),
                                   reference = "y")
    
    gcproc.predict <- gcproc.model$predict$predict.y
    
    # Extract imputed values
    y_hat.plot <- gcproc.predict[which(predict$x==1,arr.ind = T)]
    data___x.plot <- data___x[which((predict$x)==1,arr.ind = T)]
    mice.pred.plot <- mice.pred[which((predict$x)==1,arr.ind = T)]
    
    # Set up table for plotting
    vals <- data.frame(rbind(data.frame(seed,Metric="Loss: Root Mean Squared Error",
                                        data.frame(Method = c("gcproc","mice"),Value=c(
                                          mean(sqrt((y_hat.plot-data___x.plot)^2)),mean(sqrt((mice.pred.plot-data___x.plot)^2))
                                        ))),
                             data.frame(seed,Metric="Correlation: Pearson",                                data.frame(Method=c("gcproc","mice"),Value=c(
                               cor(y_hat.plot,data___x.plot),cor(mice.pred.plot,data___x.plot)))
                             )
    ))
    
    main_scores.2 <- rbind(main_scores.2,vals)
    
  }
  
  
  
  library(ggplot2)
  a <- ggplot((main_scores.2[main_scores.2$Metric=="Loss: Root Mean Squared Error",]), aes(x=Metric, y=Value, fill=Method)) + geom_violin() + ggtitle(label = predict.names[list_id]) 
  print(a)
  
  
  library(ggplot2)
  b <- ggplot((main_scores.2[main_scores.2$Metric=="Correlation: Pearson",]), aes(x=Metric, y=Value, fill=Method)) + 
    geom_violin() + ggtitle(label = predict.names[list_id]) 
  print(b)

}

```




